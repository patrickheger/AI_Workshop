{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code along notebook: Support Vector Calssifier with the Iris-Dataset\n",
    "\n",
    "Great to have you in this second phase of the workshop!\n",
    "\n",
    "Now we will have some time for hands-on building our own AI to do, what we did to start of the workshop.\n",
    "Everything done here also is in the folder \"solution\", where you can find the notebook \"iris_classifier_solved\" to look into, whenever you get stuck.\n",
    "\n",
    "Before you look up the solution: Did you already ask ChatGPT for a hint or the solution?\n",
    "\n",
    "This notebook is divided into 4 pieces:\n",
    "\n",
    "1. Load data\n",
    "2. Plot data\n",
    "3. Prepare data\n",
    "4. Train and test\n",
    "\n",
    "As you can see, the training only is the last part of this notebook. Way more time is used for the correct buildup and understanding of the data.\n",
    "In every division of this notebook you find a short summary, what should be done in that section and some clues, where to get help for the implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import seaborn as sns # plotting\n",
    "import matplotlib.pyplot as plt # plotting\n",
    "\n",
    "from sklearn.model_selection import train_test_split #to split the dataset for training and testing\n",
    "from sklearn import svm  #for Support Vector Machine (SVM) Algorithm\n",
    "from sklearn import metrics #for checking the model accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data\n",
    "\n",
    "First step is to load data. We use the library pandas for that, as this has built-in functions to read csv and xlsx. pandas is also widely supported by any AI-libraries as it is the de-facto-standard in the industry.\n",
    "\n",
    "Information on data loading with pandas (imported as \"pd\") into a __DataFrame__:\n",
    "\n",
    "https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data \n",
    "df_iris = \n",
    "\n",
    "# We can drop the ID-fielad as pandas uses its own ID\n",
    "\n",
    "\n",
    "# sanity check using the head()-function\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot data for a better understanding\n",
    "\n",
    "Plotting the data leads to a better understanding. The second plot was also used for the start into the workshop.\n",
    "\n",
    "Plotting needs quite some parameters to have a good overview of the plotted data. Therefore, the first plot stays in as code. Use it to build the second plot, that matches the one from the morning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = df_iris[df_iris.Species=='Iris-setosa'].plot(kind='scatter',x='SepalLengthCm',y='SepalWidthCm',color='red', label='Setosa', marker = \"1\")\n",
    "\n",
    "df_iris[df_iris.Species=='Iris-versicolor'].plot(kind='scatter',x='SepalLengthCm',y='SepalWidthCm',color='blue', label='Versicolor',ax=fig, marker = \"D\")\n",
    "df_iris[df_iris.Species=='Iris-virginica'].plot(kind='scatter',x='SepalLengthCm',y='SepalWidthCm',color='green', label='Virginica', ax=fig, marker = \"x\")\n",
    "\n",
    "fig.set_xlabel(\"Sepal Length\")\n",
    "fig.set_ylabel(\"Sepal Width\")\n",
    "fig.set_title(\"Sepals\")\n",
    "\n",
    "fig=plt.gcf()\n",
    "fig.set_size_inches(10,6)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The plot for the Petals comes here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a better understanding, we can build a heatmap of the correlation. 2 functions can be combined for that:\n",
    "\n",
    "1. Correlation can be calculated by the pandas-package: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.corr.html\n",
    "2. The heatmap can be plotted by seaborn (imported as sns): https://seaborn.pydata.org/generated/seaborn.heatmap.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of Correlation matrix to get a better understandig of connections in the data\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "# your code for the heatmap comes here:\n",
    "###\n",
    "\n",
    "###\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation of the heatmap:\n",
    "\n",
    "As seen in the plotted data: The longer the petals, the wider they get. This is not true for the Sepals.\n",
    "Also there seems to be a strong correlation between Petal size and Sepal length.\n",
    "Sepal width does not strongly correlate with another feature.\n",
    "\n",
    "In a real world example, this heatmap might lead to a selection of features to make the learning process faster as we have nearly perfect correlation between some features. As processing time is no concern on 150 records, we won't modify the data at this point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the data for training\n",
    "\n",
    "Now that we have some understanding of the data and our task, we can prepare it for the following training and testing.\n",
    "Best practice is to train the model on only 60%-80% of the data and use the rest of it as a test to measure, how good the model performs.\n",
    "\n",
    "We already imported \"train_test_split\". You find information on its use here in the documentation of sklearn: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "\n",
    "I recommend on using the names \"df_train\" and \"df_test\" for the training and testing data to make it easier to match the solution-notebook, if needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test set\n",
    "# the parameter test_size = 0.2 uses a random sample of 20% as a test set for validation. A random state of 1 sets a seed to make results easier to compare.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have 2 sets of training and test data, we need to make sure, to differenciate between the features (x) and the result (y). In  our case the features are the 4 different columns with numerical data and. The result is the species of the flower.\n",
    "In the next block, we should get up to 4 different dataframes:\n",
    "\n",
    "__df_train_x__: features of the training set <br>\n",
    "__df_train_y__: results of the trainig set\n",
    "\n",
    "__df_test_x__ = features of the test set <br>\n",
    "__df_test_y__ = results of the test set\n",
    "\n",
    "pandas documentation on how to split up dataframes by column: https://pandas.pydata.org/docs/user_guide/indexing.html#basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and test\n",
    "\n",
    "We use a Support Vector Machine (svm) or as this is a classification task a Support Vector Classifier (svc) as it does the same we did in the intro-task. It calculates a line between the classes\n",
    "\n",
    "More to read on SVCs: https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html <br>\n",
    "We need to build a new Support Vector classifier by using its constructor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build an instance of the Support Vector Machine\n",
    "model = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can train the model by using its \"fit\" method. This is also shown in the examples-section of the SVC-documentation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We bow use the model to predict the classes of the test data <br>\n",
    "This is done by using the \"predict\"-function of the model and feeding in the x-data of the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prediction = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we compare that to the true classes of the test set. Therefore the accuracy is a great tool, as it measures, which percentage of the test set is labled correctly.\n",
    "If you want to look into the full understanding, which metrics can be used to ensure the quality of the model, https://en.wikipedia.org/wiki/F-score (__CN statistics__) is a great place to start. The section \"Diagnostic testing\" shows a great overview over different metrics and their use cases. \n",
    "\n",
    "We may use the imported \"metrics\"-class of sklearn to calculate the accuracy_score. This method need the predictions and the true classes of every record in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "svc_accuracy = \n",
    "\n",
    "print(svc_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything worked as intended, the model labels around 97% of the test data correctly.\n",
    "\n",
    "DONE! There we have a first AI-model with a great accuracy.\n",
    "\n",
    "AI is no witchcraft! In a real world example, finding good data, understanding it and finding a good model are the main challenges for many datasets. Also \"Finetuning\" models is a way to get even better results, by dving deeper into the underlying statistics and using so called hyperparameters to fit the model even better to the use case. <br>\n",
    "For that, many models like this are built and compared to find a few contenders, which can be used in a real world test and overseen by knowledgeable users to recognize mistakes and flaws and get to a final model or another iteration of training with even better data.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time left?\n",
    "\n",
    "Let's look into, where the one label is used wrongly.\n",
    "\n",
    "For that deeper dive into the labels, a confusion matrix is the tool to use. This matrix shows, what every flower of the test set is truely labeled and how the model labeled it. Both imports are to be found in the sklearn-documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diving into the exact mistakes of the model\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# caught fire? Further information!\n",
    "- finetuning models: https://scikit-learn.org/stable/modules/grid_search.html\n",
    "\n",
    "- more algorithms and models: (https://scikit-learn.org/stable/machine_learning_map.html)\n",
    "\n",
    "- math behind the SVM (MIT Course: https://www.youtube.com/watch?v=_PwhiWxHK8o&t)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-workspace-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
